# Prism â€” Initial Version (v0.2) Technical Plan

## Overview

This folder documents the transition from **UX Preview** (mock data, simulated AI) to **Initial Version** (real local models, live tools, voice input).

## Architecture

```
lib/
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ ai_service.dart          # âœ… Created â€” AI controller abstraction
â”‚   â”œâ”€â”€ tool_registry.dart       # âœ… Created â€” Composable tools
â”‚   â”œâ”€â”€ feature_registry.dart    # âœ… Created â€” Feature availability
â”‚   â”œâ”€â”€ feature_gate.dart        # âœ… Created â€” UI gating widget
â”‚   â”œâ”€â”€ llama_controller.dart    # ðŸ”§ v0.2 â€” llama.cpp FFI binding
â”‚   â”œâ”€â”€ ollama_controller.dart   # ðŸ”§ v0.2 â€” Ollama HTTP API
â”‚   â”œâ”€â”€ cloud_controller.dart    # ðŸ”§ v0.2 â€” OpenAI/Gemini/Anthropic APIs
â”‚   â””â”€â”€ intent_parser.dart       # ðŸ”§ v0.2 â€” NL â†’ tool call mapping
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mock_data_service.dart   # âœ… Created â€” JSON asset loader
â”‚   â”œâ”€â”€ local_db.dart            # ðŸ”§ v0.2 â€” Drift/Hive local storage
â”‚   â””â”€â”€ sync_service.dart        # ðŸ“‹ v0.3 â€” Cross-device sync
â”œâ”€â”€ voice/
â”‚   â”œâ”€â”€ stt_service.dart         # ðŸ“‹ v0.3 â€” Speech-to-text
â”‚   â””â”€â”€ tts_service.dart         # ðŸ“‹ v0.3 â€” Text-to-speech
â””â”€â”€ ...
```

## Key Packages for v0.2

| Package | Purpose | Status |
|---------|---------|--------|
| `llama_sdk: ^0.0.5` | Local LLM inference via llama.cpp FFI | Planned |
| `ollama_dart: ^0.2.2` | Ollama API client | Planned |
| `dart_openai: ^5.1.0` | OpenAI-compatible APIs | Planned |
| `google_generative_ai: ^0.4.6` | Gemini API | Planned |
| `hive_ce: ^2.6.0` | Fast local key-value storage | Planned |
| `drift: ^2.22.0` | SQLite ORM for structured data | Planned |
| `dio: ^5.7.0` | HTTP client for model downloads | Planned |
| `path_provider: ^2.1.5` | File system paths | Planned |

## Local Model Loading Flow (Reference: Maid app)

```
1. User selects model in Settings â†’ AI Providers â†’ Local
2. Check if model file exists in app documents directory
3. If not: download via Dio with Stream<double> progress
4. Load model into llama.cpp via llama_sdk
5. Set as active controller in AIService
6. Ready for inference
```

### Key Implementation Details

- **Model catalog**: `assets/mock_data/models.json` defines available models
- **Download**: Use `Dio` with progress tracking, store in `getApplicationDocumentsDirectory()`
- **Hash verification**: Compare SHA-256 after download to prevent re-downloads
- **Memory management**: Only one model loaded at a time; unload before switching
- **Platform guards**: llama.cpp FFI not available on web â€” use `kIsWeb` to gate

## Tools System

Tools are composable â€” a tool can call other tools:

```dart
// Example: AI says "Mark task done and log 30 min work"
// â†’ UpdateTaskTool + AddTransactionTool (via ToolRegistry)
```

Current tools:
- `update_task` â€” Change task status/details
- `add_transaction` â€” Log financial entry
- `create_note` â€” Add Brain document
- `schedule_event` â€” Calendar entry
- `edit_document` â€” Modify existing Brain doc
- `get_weather` â€” Weather information

## Feature Gating

The `FeatureGate` widget wraps screens/sections with status banners:
- **Preview**: Yellow banner â€” "Using sample data"
- **Partial**: Yellow banner â€” "Some features still building"
- **Planned**: Placeholder page with timeline
- **Available**: No banner, direct render

## Voice-First Design

The default input method is voice:
1. Home screen shows microphone as primary input
2. Text field is secondary (tap to switch)
3. Voice â†’ STT â†’ Intent parser â†’ Tool or Chat
4. Response â†’ TTS â†’ Voice output

Voice processing chain (v0.3):
```
User speaks â†’ speech_to_text (on-device) â†’ text
text â†’ AI intent parser â†’ { tool_call | chat_response }
response â†’ flutter_tts (on-device) â†’ User hears
```
